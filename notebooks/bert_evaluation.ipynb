{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geoph/.cache/pypoetry/virtualenvs/src-GIJ9yZz_-py3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from datasets import load_from_disk, Dataset\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "from nervaluate import Evaluator\n",
    "\n",
    "from src.controllers import Controller\n",
    "from src.NER.bert.bert_hf import BertTokenClassifierHF\n",
    "from src.tools.general_tools import load_pickled_data\n",
    "from src.tools.general_tools import get_filepath, get_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dir = \"../results/evaluation/bert/checkpoint-417\"\n",
    "# assert os.path.isdir(model_dir), f'Model not found at {model_dir}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1670\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 186\n",
       "    })\n",
       "    all: Dataset({\n",
       "        features: ['ner_tags', 'original_text', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1856\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk(\"../results/dataset/bert\")\n",
    "label_names = list(load_pickled_data(\"../results/dataset/bert/labels.pkl\"))\n",
    "eval_original_texts = load_pickled_data(\"../results/dataset/bert/eval_original_text.pkl\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 18:37:53.873 | INFO     | src.NER.bert.bert_hf:_get_latest_ckpt_path:235 - Using latest checkpoint: checkpoint-834.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/geoph/Repos/AgroknowNER/foodner/results/evaluation/bert-3epochs\n"
     ]
    }
   ],
   "source": [
    "c = Controller('bert')\n",
    "\n",
    "data_dir: str = get_folder_path(c._dataset_base_path)\n",
    "bert_model = BertTokenClassifierHF(\n",
    "    dataset_base_path=data_dir, \n",
    "    eval_base_path=get_folder_path(c._evaluation_base_path+\"-3epochs\"),\n",
    "    device='cpu'\n",
    ")\n",
    "print(bert_model.output_dir)\n",
    "bert_model._use_finetuned_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use our evaluate method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'offset_mapping'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results, results_per_tag \u001b[39m=\u001b[39m bert_model\u001b[39m.\u001b[39;49mevaluate()\n",
      "File \u001b[0;32m~/Repos/AgroknowNER/foodner/src/NER/bert/bert_hf.py:149\u001b[0m, in \u001b[0;36mBertTokenClassifierHF.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39m# get the ground truths and predictions and remove the special start and end tokens\u001b[39;00m\n\u001b[1;32m    145\u001b[0m ground_truths: List \u001b[39m=\u001b[39m [\n\u001b[1;32m    146\u001b[0m     [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid2label\u001b[39m.\u001b[39mget(x, DEFAULT_NER_LABEL) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m ner_labels][\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    147\u001b[0m     \u001b[39mfor\u001b[39;00m ner_labels \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[\u001b[39m'\u001b[39m\u001b[39meval\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    148\u001b[0m ]\n\u001b[0;32m--> 149\u001b[0m predictions \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_token_classes(text)[\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m evaluations_raw_text]\n\u001b[1;32m    150\u001b[0m \u001b[39m# make length assertions\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(ground_truths) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(predictions) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(ground_truths[\u001b[39m0\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(predictions[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/Repos/AgroknowNER/foodner/src/NER/bert/bert_hf.py:149\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39m# get the ground truths and predictions and remove the special start and end tokens\u001b[39;00m\n\u001b[1;32m    145\u001b[0m ground_truths: List \u001b[39m=\u001b[39m [\n\u001b[1;32m    146\u001b[0m     [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mid2label\u001b[39m.\u001b[39mget(x, DEFAULT_NER_LABEL) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m ner_labels][\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    147\u001b[0m     \u001b[39mfor\u001b[39;00m ner_labels \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[\u001b[39m'\u001b[39m\u001b[39meval\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    148\u001b[0m ]\n\u001b[0;32m--> 149\u001b[0m predictions \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_token_classes(text)[\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m evaluations_raw_text]\n\u001b[1;32m    150\u001b[0m \u001b[39m# make length assertions\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(ground_truths) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(predictions) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(ground_truths[\u001b[39m0\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(predictions[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/Repos/AgroknowNER/foodner/src/NER/bert/bert_hf.py:190\u001b[0m, in \u001b[0;36mBertTokenClassifierHF._predict_token_classes\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    183\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(\n\u001b[1;32m    184\u001b[0m     text,\n\u001b[1;32m    185\u001b[0m     is_split_into_words\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    186\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    187\u001b[0m     truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    188\u001b[0m )\n\u001b[1;32m    189\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 190\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\u001b[39m.\u001b[39mlogits\n\u001b[1;32m    192\u001b[0m predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    193\u001b[0m predicted_token_class \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mid2label[t\u001b[39m.\u001b[39mitem()] \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m predictions[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/src-GIJ9yZz_-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'offset_mapping'"
     ]
    }
   ],
   "source": [
    "results, results_per_tag = bert_model.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With best model loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Approaches and Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"ner\", model=\"../results/evaluation/bert/checkpoint-834/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-SKIP', 'score': 0.63059723, 'index': 27, 'word': 'AN', 'start': 65, 'end': 67}, {'entity': 'I-SKIP', 'score': 0.75765026, 'index': 30, 'word': 'DR', 'start': 72, 'end': 74}, {'entity': 'B-SKIP', 'score': 0.80880636, 'index': 34, 'word': 'F', 'start': 81, 'end': 82}] Title 21: Food and Drugs PART 556-TOLERANCES FOR RESIDUES OF NEW ANIMAL DRUGS IN FOOD Subpart B-Specific Tolerances for Residues of New Animal Drugs $556.513 Piperazine. A tolerance of 0.1 part per million piperazine base is established for edible tissues of poultry and swine. [64 FR 23019, Apr. 29, 1999]\n"
     ]
    }
   ],
   "source": [
    "out = classifier(eval_original_texts[0])\n",
    "print(out[:3], eval_original_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 15:59:34.374 | INFO     | src.NER.bert.bert_hf:_get_latest_ckpt_path:99 - Using latest checkpoint: checkpoint-834.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Title', 'O'), ('21', 'O'), (':', 'O'), ('Food', 'O'), ('and', 'O'), ('Drugs', 'O'), ('PA', 'O'), ('##RT', 'O'), ('55', 'O'), ('##6', 'O'), ('-', 'O'), ('TO', 'O'), ('##LE', 'O'), ('##RA', 'O'), ('##NC', 'O'), ('##ES', 'O'), ('F', 'O'), ('##OR', 'O'), ('R', 'O'), ('##ES', 'O'), ('##ID', 'O'), ('##UE', 'O'), ('##S', 'O'), ('OF', 'O'), ('NE', 'O'), ('##W', 'O'), ('AN', 'B-SKIP'), ('##IM', 'O'), ('##AL', 'O'), ('DR', 'I-SKIP')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "def predict2(text: str, model_path: str):\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer = bert_model.tokenizer\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    # model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    bert_model._use_finetuned_model()\n",
    "    model = bert_model.model\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predictions = torch.argmax(logits, dim=2)\n",
    "    predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n",
    "    tok_text = tokenizer.tokenize(text)\n",
    "    assert len(tok_text) == len(predicted_token_class)-2, f\"{len(tok_text)} != {len(predicted_token_class)}\"\n",
    "    print(list(zip(tok_text, predicted_token_class[1: -1]))[:30])\n",
    "    return tokenizer, model\n",
    "    # return predicted_token_class\n",
    "\n",
    "model_dir_200 = \"../results/evaluation/bert/checkpoint-50\"\n",
    "model_dir_ALL = \"../results/evaluation/bert/checkpoint-834\"\n",
    "tok, model = predict2(eval_original_texts[0], model_dir_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-23 15:59:55.030 | INFO     | src.NER.bert.bert_hf:_get_latest_ckpt_path:99 - Using latest checkpoint: checkpoint-834.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-SKIP', 'O', 'O', 'I-SKIP', 'O', 'O', 'O', 'B-SKIP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-SKIP', 'I-SKIP', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Substance', 'I-Substance', 'I-Substance', 'O', 'O', 'O', 'O', 'B-Value', 'O', 'I-Value', 'B-Unit', 'I-Unit', 'I-Unit', 'B-Substance', 'I-Substance', 'I-Substance', 'I-Substance', 'O', 'O', 'O', 'B-Usage', 'I-Usage', 'I-Usage', 'I-Usage', 'I-Usage', 'I-Usage', 'I-Usage', 'I-Usage', 'I-Usage', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "token_classes = bert_model.predict_token_classes(eval_original_texts[0])\n",
    "print(token_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truths = [[bert_model.id2label.get(x, 'O') for x in ner_labels] for ner_labels in bert_model.dataset['eval']['labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371 107\n",
      "[('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'B-SKIP'), ('O', 'O'), ('O', 'O'), ('O', 'I-SKIP'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'B-SKIP'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'B-SKIP'), ('O', 'I-SKIP'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'B-Substance'), ('O', 'I-Substance'), ('O', 'I-Substance'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'B-Value'), ('O', 'O'), ('B-Substance', 'I-Value'), ('O', 'B-Unit'), ('O', 'I-Unit'), ('I-Substance', 'I-Unit'), ('O', 'B-Substance'), ('O', 'I-Substance'), ('O', 'I-Substance'), ('O', 'I-Substance'), ('O', 'O'), ('B-Substance', 'O'), ('O', 'O'), ('O', 'B-Usage'), ('I-Substance', 'I-Usage'), ('O', 'I-Usage'), ('O', 'I-Usage'), ('O', 'I-Usage'), ('O', 'I-Usage'), ('O', 'I-Usage'), ('O', 'I-Usage'), ('O', 'I-Usage'), ('O', 'O'), ('O', 'O'), ('B-CAS', 'O'), ('O', 'O'), ('O', 'O'), ('I-CAS', 'O'), ('O', 'O'), ('I-CAS', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O'), ('O', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(len(ground_truths[41]), len(token_classes))\n",
    "print(list(zip(ground_truths[41], token_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 11772,  1626,   131,  6702,  1105, 26500,  8544, 10460,  3731,\n",
       "          1545,   118, 16972, 17516,  9664, 15517,  9919,   143,  9565,   155,\n",
       "          9919,  9949, 24846,  1708, 11345, 26546,  2924, 23096, 13371, 12507,\n",
       "         22219,  2591, 13472, 15969,   143,  2346, 15609, 12859, 17482,  1204,\n",
       "           139,   118,   156, 27934,  1706,  2879,  3923,  1116,  1111, 11336,\n",
       "          5053, 21405,  1116,  1104,  1203, 10854, 26500,   109,  3731,  1545,\n",
       "           119,  4062,  1495, 12558, 19888,  1673,   119,   138, 15745,  1104,\n",
       "           121,   119,   122,  1226,  1679,  1550,  9415, 15265,  2042,  2259,\n",
       "          1110,  1628,  1111, 24525, 14749,  1104,   185,  6094, 21001,  1105,\n",
       "           188, 17679,   119,   164,  3324,   143,  2069, 11866, 16382,   117,\n",
       "         23844,   119,  1853,   117,  1729,   166,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.batch_encode_plus([eval_original_texts[0]], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tok(eval_original_texts[0], return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 107, 19]), torch.Size([1, 107]), torch.Size([1, 107]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, inputs['input_ids'].shape, predictions.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-GIJ9yZz_-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d05cb7d951b8036ae68f3a280aed36c3e6d36951248d87ebe17c17bdf4a6f2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
